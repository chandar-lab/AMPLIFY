{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from utils import load_from_hf, load_from_mila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "source = \"mila\"\n",
    "model_name = \"AMPLIFY350M\"\n",
    "model_path = \"../outputs/MILA_PLM_350M_UR100P/checkpoint/pytorch_model.pt\"\n",
    "tokenizer_path = None \n",
    "config_path = \"../outputs/MILA_PLM_350M_UR100P/checkpoint/config.yaml\"\n",
    "batch_size = 32\n",
    "device = \"cuda\"\n",
    "compile = False\n",
    "fp16 = True\n",
    "\n",
    "# Dataset\n",
    "n_samples = 1000\n",
    "seq_length = 512\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMPLIFY(\n",
       "  (encoder): Embedding(27, 960, padding_idx=0)\n",
       "  (transformer_encoder): ModuleList(\n",
       "    (0-31): 32 x EncoderBlock(\n",
       "      (q): Linear(in_features=960, out_features=960, bias=False)\n",
       "      (k): Linear(in_features=960, out_features=960, bias=False)\n",
       "      (v): Linear(in_features=960, out_features=960, bias=False)\n",
       "      (wo): Linear(in_features=960, out_features=960, bias=False)\n",
       "      (resid_dropout): Dropout(p=0, inplace=False)\n",
       "      (ffn): SwiGLU(\n",
       "        (w12): Linear(in_features=960, out_features=5120, bias=False)\n",
       "        (w3): Linear(in_features=2560, out_features=960, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (ffn_dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm_2): RMSNorm()\n",
       "  (decoder): Linear(in_features=960, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model and tokenizer\n",
    "if source == \"hf\":\n",
    "    model, tokenizer = load_from_hf(model_path, tokenizer_path, fp16=fp16)\n",
    "elif source == \"mila\":\n",
    "    model, tokenizer = load_from_mila(model_path, config_path)\n",
    "else:\n",
    "    raise Exception(\"Only 'hf' and 'mila' sources are supported, not {source}.\")\n",
    "model.to(device)\n",
    "torch.compile(model, disable=~compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [02:10<00:00,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81 ± 0.01 ms/protein\n",
      "1239.09 ± 13.74 protein/s\n",
      "0.00 ± 0.00 ms/token\n",
      "19825.47 ± 219.76 token/s\n",
      "2388 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\n",
    "random.seed(seed)\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device_type=device, dtype=torch.float16, enabled=fp16):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    times = []\n",
    "    for i in tqdm(range(n_samples + 100)):\n",
    "        x = torch.stack([tokenizer.encode(random.choices(vocab, k=seq_length), return_tensors=\"pt\").squeeze() for _ in range(batch_size)])\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # Time the forward pass (inference)\n",
    "        start = time()\n",
    "        y = model(x)\n",
    "        stop = time()\n",
    "        \n",
    "        # Burn-in period of 100 samples (benefit compiled ESM)\n",
    "        if i >= 100:\n",
    "            times.append(stop - start)\n",
    "   \n",
    "times = np.array(times)\n",
    "print(f\"{np.mean(times * 1000 / batch_size):.2f} ± {np.std(times * 1000 / batch_size):.2f} ms/protein\")\n",
    "print(f\"{np.mean(batch_size / times):.2f} ± {np.std(batch_size / times):.2f} protein/s\")\n",
    "print(f\"{np.mean(times * 1000 / (seq_length * batch_size)):.2f} ± {np.std(times * 1000 / (seq_length * batch_size)):.2f} ms/token\")\n",
    "print(f\"{np.mean(seq_length / times):.2f} ± {np.std(seq_length / times):.2f} token/s\")\n",
    "print(f\"{torch.cuda.max_memory_allocated(device=device)/1e6:.0f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
