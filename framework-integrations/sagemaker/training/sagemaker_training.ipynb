{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7971c364-1d78-4895-8b88-7cfb67e12f14",
   "metadata": {},
   "source": [
    "### Install/Update the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a623051-addb-42cc-aed8-acd174d40373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade awscli botocore sagemaker -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bafebb-b636-487e-bd12-606aff5bc93b",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b86ee18-709e-489c-bd6e-48d8d01f517c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cacf0f84-f41f-4647-bb8c-1cc77410e68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "execution_role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eae2e1-bc77-4681-ae28-d34465aec87f",
   "metadata": {},
   "source": [
    "### Define Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36344ea2-d87b-4e05-a953-7409627ca607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust this to your local folder path\n",
    "s3_data_location = \"s3://amplify-models-aws/data/uniref50/uniref50_sample_100.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a5d10-8dbd-42c2-9196-4051849280b2",
   "metadata": {},
   "source": [
    "### Define the instance type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18f6c42c-3130-4cc9-b273-bd7695bb1f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.12xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80cb9-ed17-4c42-8d29-3eb3e0630d08",
   "metadata": {},
   "source": [
    "### Define the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e59deb-4749-4b28-b7b0-0f976051c7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66f406-1d44-4520-a5c7-d4ebda269501",
   "metadata": {},
   "source": [
    "### Define the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b77e0916-a3fb-4301-bf6c-649872670866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    py_version=\"3.10\",\n",
    "    entry_point='train.py', \n",
    "    source_dir='code',        \n",
    "    role=execution_role,\n",
    "    image_uri = image_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, \n",
    "    keep_alive_period_in_seconds=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359235db-21de-4b41-b9ef-7ed28cf0fecc",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1aa1061-d257-47ad-b517-876ca810cd81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: AMPLIFY-hf-training-job-1728946972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-14 23:02:52 Starting - Starting the training job...\n",
      "2024-10-14 23:03:23 Downloading - Downloading the training image\n",
      "2024-10-14 23:03:23 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-10-14 23:03:24,077 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-14 23:03:24,114 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-14 23:03:24,126 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:03:24,128 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:03:25,652 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/chandar-lab/AMPLIFY.git (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/chandar-lab/AMPLIFY.git to /tmp/pip-req-build-4jyhpg1g\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/chandar-lab/AMPLIFY.git /tmp/pip-req-build-4jyhpg1g\u001b[0m\n",
      "\u001b[34mResolved https://github.com/chandar-lab/AMPLIFY.git to commit 4e3055779e4be625c46cecc443f2f8cf970f8f58\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting hydra-core==1.3.2 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting omegaconf<2.4,>=2.2 (from hydra-core==1.3.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core==1.3.2->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mCollecting torch==2.2.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.27.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.13.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.13.5.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting xformers==0.0.24.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.38.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.17.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb==0.16.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy==1.26.* in /opt/conda/lib/python3.10/site-packages (from amplify==0.0.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.*->amplify==0.0.0->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.*->amplify==0.0.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.24.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.10.1)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.11.1)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.6.3)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.1.4)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting triton==2.2.0 (from torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.*->amplify==0.0.0->-r requirements.txt (line 1)) (2024.7.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1)) (72.1.0)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting torch==2.2.* (from amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.*->amplify==0.0.0->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.13.*->amplify==0.0.0->-r requirements.txt (line 1)) (2.16.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.*->amplify==0.0.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.*->amplify==0.0.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.17.1-py3-none-any.whl (536 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.7/536.7 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 117.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 112.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 218.2/218.2 MB 92.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 40.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 74.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 117.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 127.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 70.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 38.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 105.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 129.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 132.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 125.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 93.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34mDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 106.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/19.7 MB 108.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: antlr4-python3-runtime, amplify, deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=f1223e8f327ccfd046d077d388ede10b6c1aaf24a53ba106c790540df198fff0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\u001b[0m\n",
      "\u001b[34mBuilding wheel for amplify (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for amplify (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for amplify: filename=amplify-0.0.0-py3-none-any.whl size=34837 sha256=b26e43959aa937ad29b89c86951d54dc2ce1938c0ba31e84f96817a1db70ef5d\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-jmbpns58/wheels/4b/23/82/efe37253149324a2064de62b6a0cb87a91175fa5bea7abbf00\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.13.5-py3-none-any.whl size=1370572 sha256=b6ccb8381bfd6d8b8068652fe19ebc472846612b5187bc1edbe362608cca1b18\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b2/0a/43/6e1f12bbb7f5cb2762aba38dc98225956ae6a4c37bf900925a\u001b[0m\n",
      "\u001b[34mSuccessfully built antlr4-python3-runtime amplify deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, hjson, appdirs, antlr4-python3-runtime, triton, smmap, setproctitle, sentry-sdk, pynvml, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, docker-pycreds, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, gitdb, nvidia-cusolver-cu12, GitPython, wandb, transformers, torch, datasets, xformers, deepspeed, accelerate, amplify\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2024.2.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2024.2.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.18.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.18.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.18.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.43 accelerate-0.27.2 amplify-0.0.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 datasets-2.17.1 deepspeed-0.13.5 docker-pycreds-0.4.0 fsspec-2023.10.0 gitdb-4.0.11 hjson-3.1.0 hydra-core-1.3.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 py-cpuinfo-9.0.0 pynvml-11.5.3 sentry-sdk-2.16.0 setproctitle-1.3.3 smmap-5.0.1 torch-2.2.0 transformers-4.38.2 triton-2.2.0 wandb-0.16.6 xformers-0.0.24\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:09,956 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:09,956 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,016 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,065 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,116 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,129 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"AMPLIFY-hf-training-job-1728946972\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-111918798052/AMPLIFY-hf-training-job-1728946972/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-111918798052/AMPLIFY-hf-training-job-1728946972/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"AMPLIFY-hf-training-job-1728946972\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-111918798052/AMPLIFY-hf-training-job-1728946972/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,131 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:10,131 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m[2024-10-14 23:05:12,756] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m{'model_parameters': 118276507}\u001b[0m\n",
      "\u001b[34mTrain:   0%|          | 0/10 [00:00<?, ?step/s]\u001b[0m\n",
      "\u001b[34mTrain:  10%|█         | 1/10 [00:00<00:05,  1.72step/s]\u001b[0m\n",
      "\u001b[34mSaving model at step 1...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  20%|██        | 2/10 [00:01<00:07,  1.02step/s]\u001b[0m\n",
      "\u001b[34mSaving model at step 2...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  30%|███       | 3/10 [00:03<00:08,  1.21s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 3...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  40%|████      | 4/10 [00:04<00:07,  1.31s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 4...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  50%|█████     | 5/10 [00:06<00:07,  1.43s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 5...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  60%|██████    | 6/10 [00:07<00:05,  1.43s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 6...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  70%|███████   | 7/10 [00:09<00:04,  1.43s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 7...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  80%|████████  | 8/10 [00:10<00:02,  1.43s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 8...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain:  90%|█████████ | 9/10 [00:12<00:01,  1.46s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 9...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain: 100%|██████████| 10/10 [00:13<00:00,  1.53s/step]\u001b[0m\n",
      "\u001b[34mSaving model at step 10...\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 2048}\u001b[0m\n",
      "\u001b[34mModel successfully saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfig successfully saved to /opt/ml/model/config.yaml\u001b[0m\n",
      "\u001b[34mTrain: 100%|██████████| 10/10 [00:15<00:00,  1.55s/step]\u001b[0m\n",
      "\n",
      "2024-10-14 23:05:54 Uploading - Uploading generated training model\u001b[34m2024-10-14 23:05:52,035 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:52,035 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-14 23:05:52,035 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-10-14 23:06:15 Completed - Resource retained for reuse\n",
      "Training seconds: 180\n",
      "Billable seconds: 180\n"
     ]
    }
   ],
   "source": [
    "training_job_name = f\"AMPLIFY-hf-training-job-{int(time.time())}\"\n",
    "\n",
    "\n",
    "estimator.fit({\n",
    "    'train': s3_data_location\n",
    "}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7ea0b-92f2-4705-8d68-87d4cdc6ebaa",
   "metadata": {},
   "source": [
    "### Get the data model data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "971fc666-9bc3-4797-9513-a7105858fbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-10-14 23:06:15 Starting - Found matching resource for reuse\n",
      "2024-10-14 23:06:15 Downloading - Downloading the training image\n",
      "2024-10-14 23:06:15 Training - Training image download completed. Training in progress.\n",
      "2024-10-14 23:06:15 Uploading - Uploading generated training model\n",
      "2024-10-14 23:06:15 Completed - Resource retained for reuse\n"
     ]
    }
   ],
   "source": [
    "estimator = HuggingFace.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9947106-bb42-4b6c-b328-462269e24d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-111918798052/AMPLIFY-hf-training-job-1728946972/output/model.tar.gz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d4184-bfd7-4aa7-95ba-facd66d54c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
